!mkdir -p "mbart"
%cd mbart

!pip install transformers
!pip install datasets
!pip install sacrebleu
!pip install sentencepiece

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import get_linear_schedule_with_warmup
from datasets import load_dataset, load_metric
import torch
from sacrebleu import corpus_bleu
import pandas as pd
import unicodedata
from datasets import Dataset

# Tokenizer
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

# Load data from files
with open("train.en", encoding="utf-8") as train_en, \
     open("train.hi", encoding="utf-8") as train_hi:

    # Read the lines from each file
    lines_en = train_en.readlines()
    lines_hi = train_hi.readlines()

    # Combine the lines into pairs and preprocess them
    data = [(hi.strip(), en.strip()) for hi, en in zip(lines_hi, lines_en)]

# Create a pandas DataFrame
train_df = pd.DataFrame(data, columns=["input_text","target_text"])

# Load data from files
with open("test.en", encoding="utf-8") as test_en, \
     open("test.hi", encoding="utf-8") as test_hi:

    # Read the lines from each file
    lines_en = test_en.readlines()
    lines_hi = test_hi.readlines()

    # Combine the lines into pairs and preprocess them
    data = [(hi.strip(), en.strip()) for hi, en in zip(lines_hi, lines_en)]

# Create a pandas DataFrame
test_df = pd.DataFrame(data, columns=["input_text","target_text"])

# Load data from files
with open("valid.en", encoding="utf-8") as valid_en, \
     open("valid.hi", encoding="utf-8") as valid_hi:

    # Read the lines from each file
    lines_en = valid_en.readlines()
    lines_hi = valid_hi.readlines()

    # Combine the lines into pairs and preprocess them
    data = [(hi.strip(), en.strip()) for hi, en in zip(lines_hi, lines_en)]

# Create a pandas DataFrame
valid_df = pd.DataFrame(data, columns=["input_text","target_text"])

#print(train_df)
#print(test_df)
#print(valid_df)

# Convert DataFrames to Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)
valid_dataset = Dataset.from_pandas(valid_df)


#Tokenize function
def tokenize_data(example):
    input = tokenizer(example["input_text"], max_length=128, truncation=True, padding="max_length")
    target = tokenizer(example["target_text"], max_length=128, truncation=True, padding="max_length")
    return {"input_ids": input["input_ids"],
            "attention_mask": input["attention_mask"],
            "labels": target["input_ids"]}


# Map the tokenization function to the datasets
tokenized_train_dataset = train_dataset.map(tokenize_data, batched=True, remove_columns=["input_text", "target_text"])
tokenized_valid_dataset = valid_dataset.map(tokenize_data, batched=True, remove_columns=["input_text", "target_text"])

# Model
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    evaluation_strategy="epoch",
    logging_dir="./logs",
    save_strategy="epoch",
    num_train_epochs=3,
    learning_rate=1e-5, # lower learning rate
    weight_decay=0.03,
    warmup_steps=2000, # increase warmup steps
    save_total_limit=3,
    lr_scheduler_type="linear", # use linear learning rate schedule
    load_best_model_at_end=True,
    metric_for_best_model="loss", # use validation loss for early stopping
)


# Initialize the trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_valid_dataset,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Load the fine-tuned model and tokenizer
model = MBartForConditionalGeneration.from_pretrained("./results/cmu_phinc_hinge_context_clean_more_3e/checkpoint-8583") 
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

# # Load data from files
# test_data = pd.read_csv("test.hi", sep="\t", header=None, names=["input_text"]).applymap(preprocess_text)
# test_labels = pd.read_csv("test.en", sep="\t", header=None, names=["target_text"]).applymap(preprocess_text)

# # Extract Hinglish and English sentences
# test_hinglish = test_data["input_text"].tolist()
# test_english = test_labels["target_text"].tolist()

# Load data from files
with open("test.en", encoding="utf-8") as test_en, \
     open("test.hi", encoding="utf-8") as test_hi:

    # Read the lines from each file
    lines_en = test_en.readlines()
    lines_hi = test_hi.readlines()

    test_hinglish = [hi.strip() for hi in lines_hi]
    test_english = [en.strip() for en in lines_en]

# Reduce the size of the test data (e.g., use only the first 100 sentences)
test_hinglish_subset = test_hinglish[:15]
test_english_subset = test_english[:15]

# # Generate translations
translations = []
for text in test_hinglish:
    inputs = tokenizer(text, return_tensors="pt", max_length=128, truncation=True)
    outputs = model.generate(**inputs)
    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
    translations.append(translation)


for i in range(15):
  print("Test : ",test_english[i])
  print("Translation : ",translations[i])

# Calculate sacreBLEU score
bleu_score = corpus_bleu(translations, [test_english]).score
print(f'sacreBLEU score: {bleu_score}')
